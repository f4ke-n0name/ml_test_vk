# ml_test_vk
# Сводка подхода к поиску коротких заставок в сериалах

## Общий подход
Цель проекта — автоматическое определение начала (`start`) и конца (`end`) коротких заставок в сериалах с использованием аудиоанализа. Почему аудиоанализ? После некоторых попыток как-то придумать визуальные признаки, которые помогают модели предсказать таргет, причем чтобы они не занимали много памяти, я отказался от визуального анализа из-за низкой эффективности, ввиду того что заставки разбросаны в разных частях и тяжело строить модель, которая будет анализировать куски видео (для меня), и сосредоточился на аудиоданных.

Пока сделано следующее:

- **Обработка аудио**:
   - **Преобразование метаданных**: Обработка `train_labels.json` и `test_labels.json` с преобразованием меток (`start`, `end`) в секунды (`start_sec`, `end_sec`) и вычислением длительности (`duration`). Анализ в каких пределах находиться заставка - результат: в первых 5 минут (анализ метаданных)
   - Извлечение аудио из первых 5 минут (300 секунд) каждого видео с помощью `ffmpeg` в папку `audio/`, что уменьшает объем данных
   - Подготовка аудиопризнаков: полные массивы MFCC и спектрального контраста с увеличенным `hop_length=1024` для оптимизации, сохраненные в `audio_features/` в формате JSON
   - Создание `mapping.json` с путями к видео, аудио и файлам признаков для упрощения доступа

- **Проектирование сырых признаков для модели**:
   - **Временные признаки**: Полные массивы MFCC и спектрального контраста для 10-секундных сегментов аудио, сегментированных с шагом 5 секунд (~22 сегмента на аудио)
   - **Дополнительные признаки**: Громкостные (RMS), спектральный центроид, разносторонность как 1D-временные ряды; темп, вариация битов и кросс-корреляция как скалярные метрики для оценки повторяемости
   - **Разметка**: Сегменты маркируются как "заставка" (1) или "не заставка" (0) на основе `train_labels.json`, с возможностью регрессии для точных границ

## Архитектура модели (набросок)

Были рассмотрены следующие модели:

1. **CNN**:
  - **Причина**: Подходит для обработки 2D-данных для MFCC и спектральный контраста
  - **Слои**: 
    - Входной слой принимает сегменты аудио (например, 10 секунд с ~50 кадрами)
    - Несколько сверточных слоев (например, 32-64 фильтра, ядро 3x3) с функцией активации ReLU для извлечения локальных паттернов
    - Пулинг-слои (max pooling) для уменьшения размерности
    - Полносвязные слои для классификации или регрессии

2. **RNN или LSTM**:
  - **Причина**: Подходит для последовательного анализа временных рядов
  - **Слои**: 
    - Входной слой принимает 1D- или 2D-признаки по времени
    - 1-2 слоя LSTM для моделирования долгосрочных зависимостей
    - Полносвязный слой для финального предсказания

Итого, я пришел к выводу, что наилучшей моделью будет CNN + RNN 

- **Причина**: Комбинирует обработку 2D-признаков (CNN) и временной последовательности (RNN)
- **Слои**: CNN сначала извлекает признаки из MFCC/контраста, затем LSTM анализирует последовательность
- **Преимущества**: Максимальная гибкость для сложных аудиопаттернов

### Выходные данные
- **Комбинированный подход**: Модель сначала классифицирует, а затем уточняет границы с регрессией

### Параметры обучения
- **Оптимизатор**: Adam с начальной скоростью обучения 0.001, с возможным уменьшением
- **Функция потерь**:
  - Классификация: кросс-энтропия
  - Регрессия: среднеквадратичная ошибка (MSE)
  - Комбинированный: взвешенная сумма потерь
- **Регуляризация**: Dropout и L2-регуляризация для предотвращения переобучения
- **Гиперпараметры**: Число сверточных фильтров (32-128), размер пулинга (2x2), число LSTM-единиц (64-128), оптимизируются через кросс-валидацию

### Оценка
- **Метрики**: 
  - Классификация: точность (accuracy), F1-score
  - Регрессия: средняя абсолютная ошибка (MAE) для `start_sec` и `end_sec`
- **Валидация**: Кросс-валидация на 5 фолдов или hold-out на `test_labels.json`

## Что делать дальше
- **Подготовка обучающего набора (набросок)**:
  - Сегментировать аудиофайлы в 10-секундные отрезки с шагом 5 секунд и присвоить метки на основе `train_labels.json`
  - Сформировать финальный набор признаков для каждого сегмента
  - Классификация: "заставка" (1), если центр сегмента в `[start_sec, end_sec]` из `train_labels.json`, иначе "не заставка" (0)

- **Анализ и оптимизация признаков**:
  - Провести предварительный анализ признаков для оценки их информативности
  - Оптимизировать параметры (например, `hop_length` или длину сегмента), чтобы сбалансировать объем данных и качество
 
## Работа с новыми сериалами:
- **Подготовка данных**: Для новых сериалов повторяем процесс подготовки
- **Предсказание с обученной моделью**: Применяем обученную CNN+RNN модель к новым аудиосегментам, используя те же признаки. Модель сначала классифицирует сегменты как потенциальные заставки, а затем уточняет границы (`start_sec`, `end_sec`) с помощью регрессии
- **Анализ без разметки (дополнительно)**: В случае низкой уверенности модели (например, вероятность < 0.7) или отсутствия разметки используем кластеризацию (например, DBSCAN) на основе MFCC и спектрального контраста для выявления повторяющихся сегментов, которые могут указывать на заставки
- **Ручная валидация и дообучение**: Проверяем предсказанные сегменты вручную, подтверждая или корректируя их как заставки. Собранные данные с разметкой интегрируем в обучающий набор для дообучения модели
